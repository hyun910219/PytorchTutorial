{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "- https://pytorch.org/docs/0.4.0/nn.html\n",
    "- https://pytorch.org/tutorials/\n",
    "- https://ratsgo.github.io/machine%20learning/2017/10/12/terms/\n",
    "- https://github.com/DSKSD/Pytorch_Fast_Campus_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T10:26:05.885090Z",
     "start_time": "2018-08-11T10:26:05.874148Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a PyTorch?\n",
    "- Tensor와 Optimizer, Neural Net등 GPU연산에 최적화된 모듈을 이용하여 빠르게 딥러닝 모델을 구현할 수 있는 프레임워크\n",
    "Facebook이 밀고 있던 lua기반의 torch를 python버전으로 포팅함.\n",
    "\n",
    "# > Pytorch Basic\n",
    "- Tensor\n",
    "- autograd\n",
    "- nn\n",
    "- optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. optimizer\n",
    "### 4.1 Loss\n",
    "- 최적화를 하기 이전에 Loss를 정의해야함.\n",
    "- Loss는 output(결과값 or 예측값)과 target(실제값)가 얼마나 차이가 나는지 나타내는 측도(measure)로 목적에 맞게 loss function을 정의해야함.\n",
    "    - example1. 몸무게를 예측하는 경우 MSELoss를 계삼함\n",
    "    - examapl2. 꽃의 종류를 예측하는 경우 CrossEntorpy를 계산함\n",
    "    \n",
    "- torch.nn 패키지에 Loss를 계산해주는 다양한 loss function들이 있음.\n",
    "    - L1Loss\n",
    "    - MSELoss\n",
    "    - CrossEntorpyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 nn.MSELoss\n",
    "- input과 target에 대한 Mean Sqaured Error를 계산함.\n",
    "$$ Loss(x,y) = \\frac{1}{n}\\sum_{i}(pred_{i}-target_{i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:03:38.501166Z",
     "start_time": "2018-08-11T11:03:38.495396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.MSELoss() \n",
    "preds= torch.FloatTensor([[1,2],[2,0]])\n",
    "targets= torch.ones(2,2)\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 CrossEntropyLoss\n",
    "- Classification 문제에서 주로 사용되는 loss function으로 두 확률분포 사이의 차이를 재는 측도 중 하나임.\n",
    "- input과 target에 대한 CrossEntropyLoss를 계산함.\n",
    "- input은 FloatTensor type으로 클래스 수만큼 길이인 array로 구성됨.\n",
    "- target은 LongTensor로 정답인 클래스의 index로 구성됨.\n",
    "- CrossEntropy:\n",
    "$$ H(Target,Pred)= - \\sum_{x}Target(x)\\log(Q(x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:20:10.575901Z",
     "start_time": "2018-08-11T11:20:10.570072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9037)\n"
     ]
    }
   ],
   "source": [
    "loss_function= nn.CrossEntropyLoss()\n",
    "preds= torch.FloatTensor([[0.7, 1.2, 0.6], [0.3, 1, 0.8]])\n",
    "targets= torch.LongTensor([1, 2])\n",
    "loss= loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 optim\n",
    "- torch.optim 패키지를 이용해 모델의 output(결과값)과 target(실제값)에 차이인 loss를 줄이기 위해 parameter update를 통해 최적화함.  \n",
    "- 'Gradient Descent'라는 방법을 사용해 neural network의 parameter update를 함.  \n",
    "$ param : \\theta$ , $ learning rate: \\alpha$ 라고 하면 다음과 같이 업데이트 함.$$ \\theta = \\theta - \\alpha * \\frac{\\partial{loss}}{\\partial{\\theta}}$$\n",
    "- torch.optim은 다양한 optimizer를 가짐.\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adam\n",
    "    \n",
    "- 참고 사이트  \n",
    "    http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:46.495316Z",
     "start_time": "2018-08-11T11:38:46.483964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preds] tensor([[-0.2602],\n",
      "        [-0.0370],\n",
      "        [-1.4806],\n",
      "        [ 0.7957],\n",
      "        [-0.2482]])\n",
      "[loss] tensor(1.4605)\n"
     ]
    }
   ],
   "source": [
    "inputs= torch.randn(5,3)\n",
    "targets= torch.randn(5).unsqueeze(1)\n",
    "\n",
    "# model, loss function, optimizer 인스턴스를 생성\n",
    "model= nn.Linear(3,1)\n",
    "loss_function= nn.MSELoss()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "preds= model(inputs)\n",
    "print('[preds]', preds)\n",
    "loss=loss_function(preds, targets)\n",
    "print('[loss]',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:47.539257Z",
     "start_time": "2018-08-11T11:38:47.530426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward 전\n",
      "None\n",
      "None\n",
      "backward 후\n",
      "tensor([[-0.4571, -0.8702, -1.1673]])\n",
      "tensor([ 1.3773])\n"
     ]
    }
   ],
   "source": [
    "print('backward 전')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "# 각 parameter에 대한 loss에 미분값을 계산\n",
    "loss.backward()\n",
    "\n",
    "print('backward 후')\n",
    "for p in model.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전과 후를 비교해보면 각 parameter - grad * learning rate로 업데이트 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T11:38:48.404034Z",
     "start_time": "2018-08-11T11:38:48.394404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer 전\n",
      "tensor([[-0.0017,  0.0829, -0.5381]])\n",
      "tensor([-0.3557])\n",
      "optimizer 전\n",
      "tensor([[ 0.0440,  0.1699, -0.4213]])\n",
      "tensor([-0.4934])\n"
     ]
    }
   ],
   "source": [
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)\n",
    "\n",
    "# optimizer로 최적화함\n",
    "optimizer.step()\n",
    "\n",
    "print('optimizer 전')\n",
    "for p in model.parameters():\n",
    "    print(p.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
